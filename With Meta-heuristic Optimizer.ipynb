{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGyo7qCeg5bA",
    "outputId": "a34d9598-09c4-4b14-a264-f3ee3b96451d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: niapy in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from niapy) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from niapy) (1.26.4)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from niapy) (3.1.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from niapy) (2.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (2.8.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl<4.0.0,>=3.1.2->niapy) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.1.1->niapy) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.1.1->niapy) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.8.0->niapy) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install niapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHbIqZ4SCw3r"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Sphere\n",
    "from niapy.problems import Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGCKEu99Cyip",
    "outputId": "3b302be1-ddb4-42f2-f67c-92a5fbc4d564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "hkkpx6GbCyjw",
    "outputId": "41fcb93c-a5bf-4cdf-d34d-4060137fff74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 1,600\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"deceptive\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"truthful\",\n          \"deceptive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hotel\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"james\",\n          \"homewood\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"polarity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MTurk\",\n          \"Web\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"My husband and I recently went to Chicago for my Aunt's funeral. We are not familiar with the area so we asked for recommedations of where to stay for the two nights we would be in town. We were told to stay at the Ambassador East Hotel. I wish we could say that our experience there was a good one, but sadly, it was one of of the more disappointing travel experiences we have had. Upon check in we were quickly shown to our rooms, but when we entered our king suite a very strong, musty smell greeted us on the other side. At first I thought I was just being sensitive, but after about an hour in the room we simply had to request to be moved. The smell was too overwhelming. The front desk said they would be happy to move us, but the only other room available was a suite with two double beds, rather than the king bed suite we originally reserved. While this was not exactly what we wanted, we didn't really have a choice to refuse. After transfering to the new room and getting settled, we discovered that the electrical outlet near the sink in the bathroom did not work. While this did not bother my husband too much, I was very annoyed, as I need the mirror while I use my hair dryer, curling iron, etc. I absolutely did not want to have to switch rooms yet again, so I simply had to make due with doing my hair next to the bed. They also only gave us two towels of any kind in the bathroom, so we had to request washcloths and small hand towels, BOTH DAYS we were there! Each time the housekeeping staff seemed incredibly annoyed at these requests. For what we paid to stay at this hotel, we were not impressed at all. The customer service was lackluster and the rooms were not up to par. If you are ever in Chicago, do yourself a favor and stay somewhere else.\\n\",\n          \"I called on December 6th, 2010 at 3 p.m. to book a room for an upcoming weekend getaway, and was taken aback when the phone was picked up and promptly hung up without a word. Thinking this might have been a simple mistake, I gave it another try. A woman introducing herself as 'Toni' answered the phone, rushed through her introduction, sounding incredibly bored and annoyed that she had to answer the phone. Barely letting a second go by after her introduction, she gave a snotty 'Hello?!' before I could open my mouth and tell her 'Never mind.' Never has a hotel answered my call so rudely. I'd like to give the hotel the benefit of the doubt and pin this unpleasant experience to 'Toni'; however I will not be calling back again in hopes to stay at this hotel during this trip.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d3f938c1-c4eb-4f61-a9c6-937fc7098670\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>hotel</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>james</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>My stay at the James Hotel in Chicago was fant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>truthful</td>\n",
       "      <td>homewood</td>\n",
       "      <td>negative</td>\n",
       "      <td>Web</td>\n",
       "      <td>I called on December 6th, 2010 at 3 p.m. to bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>sheraton</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>I stayed at the Sheraton Chicago Hotel and Tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>truthful</td>\n",
       "      <td>james</td>\n",
       "      <td>negative</td>\n",
       "      <td>Web</td>\n",
       "      <td>While the James was once considered an elegant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>sheraton</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>Thanks Sheraton Towers for the invite to enjoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>truthful</td>\n",
       "      <td>palmer</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>Spent three nights at the hotel for a girls we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>hardrock</td>\n",
       "      <td>negative</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>I would not reccomend staying at the Hard Rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>intercontinental</td>\n",
       "      <td>negative</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>I've never written a review before, but I just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>ambassador</td>\n",
       "      <td>negative</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>My husband and I recently went to Chicago for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>hardrock</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>I traveled to Chicago recently with my giant G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3f938c1-c4eb-4f61-a9c6-937fc7098670')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d3f938c1-c4eb-4f61-a9c6-937fc7098670 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d3f938c1-c4eb-4f61-a9c6-937fc7098670');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-a599071c-896d-4a63-9a03-d62ac14be2d1\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a599071c-896d-4a63-9a03-d62ac14be2d1')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-a599071c-896d-4a63-9a03-d62ac14be2d1 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      deceptive             hotel  polarity       source  \\\n",
       "707   deceptive             james  positive        MTurk   \n",
       "884    truthful          homewood  negative          Web   \n",
       "530   deceptive          sheraton  positive        MTurk   \n",
       "1089   truthful             james  negative          Web   \n",
       "481   deceptive          sheraton  positive        MTurk   \n",
       "375    truthful            palmer  positive  TripAdvisor   \n",
       "1410  deceptive          hardrock  negative        MTurk   \n",
       "1524  deceptive  intercontinental  negative        MTurk   \n",
       "1396  deceptive        ambassador  negative        MTurk   \n",
       "632   deceptive          hardrock  positive        MTurk   \n",
       "\n",
       "                                                   text  \n",
       "707   My stay at the James Hotel in Chicago was fant...  \n",
       "884   I called on December 6th, 2010 at 3 p.m. to bo...  \n",
       "530   I stayed at the Sheraton Chicago Hotel and Tow...  \n",
       "1089  While the James was once considered an elegant...  \n",
       "481   Thanks Sheraton Towers for the invite to enjoy...  \n",
       "375   Spent three nights at the hotel for a girls we...  \n",
       "1410  I would not reccomend staying at the Hard Rock...  \n",
       "1524  I've never written a review before, but I just...  \n",
       "1396  My husband and I recently went to Chicago for ...  \n",
       "632   I traveled to Chicago recently with my giant G...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"/content/ott-deceptive-opinion.csv\",encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLflwe6ACymD"
   },
   "outputs": [],
   "source": [
    "text=df['text']\n",
    "label = df['deceptive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtWqLbLsCyno",
    "outputId": "6f29bf58-dda4-45f7-f347-e64c0e55aac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple A rate with upgrade to view room was less than $200 which also included breakfast vouchers. Had a great view of river, lake, Wrigley Bldg. & Tribune Bldg. Most major restaurants, Shopping, Sightseeing attractions within walking distance. Large room with a very comfortable bed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (text[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh2xQKKCCypo",
    "outputId": "d9e03d8d-bf76-4caf-ef53-8a9279cadb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        truthful\n",
      "1        truthful\n",
      "2        truthful\n",
      "3        truthful\n",
      "4        truthful\n",
      "          ...    \n",
      "1595    deceptive\n",
      "1596    deceptive\n",
      "1597    deceptive\n",
      "1598    deceptive\n",
      "1599    deceptive\n",
      "Name: deceptive, Length: 1600, dtype: object\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yt = le.fit_transform(label)\n",
    "print(label)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoCfxpj0Cyqy",
    "outputId": "d1caab23-6a62-4667-d407-564dec1e17d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deceptive    0\n",
      "hotel        0\n",
      "polarity     0\n",
      "source       0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9RDKmqECytf",
    "outputId": "78528f8e-451b-473e-bcd9-b35dcaf27fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import DebertaTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sb-RRPNKCyuq",
    "outputId": "4b0227b1-d055-4b54-d0df-7163b994bfd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  We stayed for a one night getaway with family on a thursday. Triple AAA rate of 173 was a steal. 7th floor room complete with 44in plasma TV bose stereo, voss and evian water, and gorgeous bathroom(no tub but was fine for us) Concierge was very helpful. You cannot beat this location... Only flaw was breakfast was pricey and service was very very slow(2hours for four kids and four adults on a friday morning) even though there were only two other tables in the restaurant. Food was very good so it was worth the wait. I would return in a heartbeat. A gem in chicago... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARI-zEJrCyxJ",
    "outputId": "d9417bbf-fe1e-469a-c201-ef1fac6fbc0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['We', 'Ġstayed', 'Ġfor', 'Ġa', 'Ġone', 'Ġnight', 'Ġget', 'away', 'Ġwith', 'Ġfamily', 'Ġon', 'Ġa', 'Ġth', 'ursday', '.', 'ĠTriple', 'ĠAAA', 'Ġrate', 'Ġof', 'Ġ173', 'Ġwas', 'Ġa', 'Ġsteal', '.', 'Ġ7', 'th', 'Ġfloor', 'Ġroom', 'Ġcomplete', 'Ġwith', 'Ġ44', 'in', 'Ġplasma', 'ĠTV', 'Ġb', 'ose', 'Ġstereo', ',', 'Ġv', 'oss', 'Ġand', 'Ġev', 'ian', 'Ġwater', ',', 'Ġand', 'Ġgorgeous', 'Ġbathroom', '(', 'no', 'Ġtub', 'Ġbut', 'Ġwas', 'Ġfine', 'Ġfor', 'Ġus', ')', 'ĠConc', 'ier', 'ge', 'Ġwas', 'Ġvery', 'Ġhelpful', '.', 'ĠYou', 'Ġcannot', 'Ġbeat', 'Ġthis', 'Ġlocation', '...', 'ĠOnly', 'Ġflaw', 'Ġwas', 'Ġbreakfast', 'Ġwas', 'Ġpricey', 'Ġand', 'Ġservice', 'Ġwas', 'Ġvery', 'Ġvery', 'Ġslow', '(', '2', 'hours', 'Ġfor', 'Ġfour', 'Ġkids', 'Ġand', 'Ġfour', 'Ġadults', 'Ġon', 'Ġa', 'Ġfr', 'iday', 'Ġmorning', ')', 'Ġeven', 'Ġthough', 'Ġthere', 'Ġwere', 'Ġonly', 'Ġtwo', 'Ġother', 'Ġtables', 'Ġin', 'Ġthe', 'Ġrestaurant', '.', 'ĠFood', 'Ġwas', 'Ġvery', 'Ġgood', 'Ġso', 'Ġit', 'Ġwas', 'Ġworth', 'Ġthe', 'Ġwait', '.', 'ĠI', 'Ġwould', 'Ġreturn', 'Ġin', 'Ġa', 'Ġheartbeat', '.', 'ĠA', 'Ġgem', 'Ġin', 'Ġch', 'icago', '...', 'Ġ', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized: ', tokenizer.tokenize(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzIRJN7CCyys",
    "outputId": "ffc7a6a1-3096-4007-d25e-5ed14eae0ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  [170, 4711, 13, 10, 65, 363, 120, 4384, 19, 284, 15, 10, 3553, 46806, 4, 8424, 17147, 731, 9, 30011, 21, 10, 8052, 4, 262, 212, 1929, 929, 1498, 19, 3550, 179, 29051, 1012, 741, 3876, 31436, 6, 748, 5434, 8, 7630, 811, 514, 6, 8, 12058, 8080, 1640, 2362, 17465, 53, 21, 2051, 13, 201, 43, 21657, 906, 1899, 21, 182, 7163, 4, 370, 1395, 1451, 42, 2259, 734, 4041, 22892, 21, 7080, 21, 26428, 8, 544, 21, 182, 182, 2635, 1640, 176, 21719, 13, 237, 1159, 8, 237, 3362, 15, 10, 6664, 21746, 662, 43, 190, 600, 89, 58, 129, 80, 97, 9248, 11, 5, 2391, 4, 3652, 21, 182, 205, 98, 24, 21, 966, 5, 2067, 4, 38, 74, 671, 11, 10, 28288, 4, 83, 15538, 11, 1855, 42938, 734, 1437, 50118]\n"
     ]
    }
   ],
   "source": [
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxhVCyzQCy0-"
   },
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in text:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent[:512],                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zf_9HwDFCy2O",
    "outputId": "c7aef3e5-aabd-49d4-d340-f98b0c7aaf73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Triple A rate with upgrade to view room was less than $200 which also included breakfast vouchers. Had a great view of river, lake, Wrigley Bldg. & Tribune Bldg. Most major restaurants, Shopping, Sightseeing attractions within walking distance. Large room with a very comfortable bed. \n",
      "\n",
      "Token IDs: [1, 35587, 8293, 83, 731, 19, 6662, 7, 1217, 929, 21, 540, 87, 68, 2619, 61, 67, 1165, 7080, 25137, 4, 7301, 10, 372, 1217, 9, 4908, 6, 8037, 6, 305, 7638, 607, 163, 4779, 571, 4, 359, 7990, 163, 4779, 571, 4, 1993, 538, 4329, 6, 21261, 6, 31833, 33471, 16530, 624, 3051, 4472, 4, 13769, 929, 19, 10, 182, 3473, 3267, 4, 1437, 50118, 2]\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', text[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbXqAVyuCy4g",
    "outputId": "004c27d4-1792-4617-9de7-06493c8db517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  143\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7Qw60HzCy7a",
    "outputId": "610888eb-4b8f-4676-f053-728d757a8fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 256 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# I've chosen 512 somewhat arbitrarily. It's slightly larger than the\n",
    "# maximum training sentence length ...\n",
    "MAX_LEN = 256\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfPo5WofDngZ"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "\n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0Z8WUNhDnhq"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for\n",
    "# training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, yt,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, yt,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lP7JuqocDnj3"
   },
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype\n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "IprzF1MoXnXM",
    "outputId": "e831c615-ed28-46ff-e5d8-5ec97497a1cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'# Find the minimum size\\nmin_size = min(train_inputs.size(0), train_masks.size(0), train_labels.size(0))\\n\\n# Trim the tensors to the minimum size\\ntrain_inputs = train_inputs[:min_size]\\ntrain_masks = train_masks[:min_size]\\ntrain_labels = train_labels[:min_size]\\n\\n# Create the DataLoader for the training set\\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\\ntrain_sampler = RandomSampler(train_data)\\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\\n\\n\\nmin_size = min(validation_inputs.size(0), validation_masks.size(0), validation_labels.size(0))\\n\\n# Trim the tensors to the minimum size\\nvalidation_inputs = validation_inputs[:min_size]\\nvalidation_masks = validation_masks[:min_size]\\nvalidation_labels = validation_labels[:min_size]\\n\\n# Create the DataLoader for the training set\\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\nvalidation_sampler = RandomSampler(validation_data)\\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Find the minimum size\n",
    "min_size = min(train_inputs.size(0), train_masks.size(0), train_labels.size(0))\n",
    "\n",
    "# Trim the tensors to the minimum size\n",
    "train_inputs = train_inputs[:min_size]\n",
    "train_masks = train_masks[:min_size]\n",
    "train_labels = train_labels[:min_size]\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "min_size = min(validation_inputs.size(0), validation_masks.size(0), validation_labels.size(0))\n",
    "\n",
    "# Trim the tensors to the minimum size\n",
    "validation_inputs = validation_inputs[:min_size]\n",
    "validation_masks = validation_masks[:min_size]\n",
    "validation_labels = validation_labels[:min_size]\n",
    "\n",
    "# Create the DataLoader for the training set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHZ0DJSVDnlD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it\n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQs2d6zcDnng"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nYmCbsbu9PF"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "FK5AzNQDzMxq",
    "outputId": "2137e1ed-fdc5-446e-a64c-6a2d5e9519b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'import torch\\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\\nfrom transformers import AdamW, get_linear_schedule_with_warmup, DebertaForSequenceClassification\\nfrom niapy.task import Task\\nfrom niapy.problems import Problem\\nfrom niapy.algorithms.basic import GreyWolfOptimizer,FireflyAlgorithm, HarrisHawksOptimization, MonarchButterflyOptimization, LionOptimizationAlgorithm, BatAlgorithm, BeesAlgorithm, BacterialForagingOptimization, CoralReefsOptimization,  CuckooSearch, CatSwarmOptimization, ForestOptimizationAlgorithm, GlowwormSwarmOptimization, MothFlameOptimizer, ParticleSwarmOptimization\\n\\n# Define GWO problem for hyperparameter optimization\\nclass HyperparameterOptimization(Problem):\\n    def __init__(self):\\n        # Dimension 2: learning rate and optimizer type\\n        super().__init__(dimension=2, lower=[1e-6, 0], upper=[1e-3, 6], type=\\'float\\')\\n\\n    def _evaluate(self, x):\\n        learning_rate = x[0]\\n        optimizer_choice = int(round(x[1]))\\n\\n        # Choose optimizer\\n        if optimizer_choice == 0:\\n            optimizer_class = torch.optim.AdamW\\n        elif optimizer_choice == 1:\\n            optimizer_class = torch.optim.SGD\\n        elif optimizer_choice == 2:\\n            optimizer_class = torch.optim.Adam\\n        elif optimizer_choice == 3:\\n            optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\\n        elif optimizer_choice == 4:\\n            optimizer_class = torch.optim.NAdam\\n        elif optimizer_choice == 5:\\n            optimizer_class = torch.optim.RMSprop\\n        elif optimizer_choice == 6:\\n            optimizer_class = torch.optim.Adagrad\\n\\n\\n\\n        # Load the DeBERTa model\\n        model = DebertaForSequenceClassification.from_pretrained(\\n            \"microsoft/deberta-base\",\\n            num_labels=2,\\n            output_attentions=False,\\n            output_hidden_states=False,\\n        )\\n        model.cuda()\\n\\n        # Optimizer and scheduler\\n        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\\n        total_steps = len(train_dataloader) * 1  # 1 epoch for quick evaluation\\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\\n\\n        # Training\\n        model.train()\\n        total_loss = 0\\n        for batch in train_dataloader:\\n            b_input_ids = batch[0].to(device)\\n            b_input_mask = batch[1].to(device)\\n            b_labels = batch[2].to(device)\\n            model.zero_grad()\\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\\n            loss = outputs.loss\\n            total_loss += loss.item()\\n            loss.backward()\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n            optimizer.step()\\n            scheduler.step()\\n\\n        avg_train_loss = total_loss / len(train_dataloader)\\n\\n        return avg_train_loss\\n\\n# Run GWO for hyperparameter optimization\\ntask = Task(problem=HyperparameterOptimization(), max_evals=10)\\nalgo = FireflyAlgorithm()\\nbest = algo.run(task)\\n\\nbest_learning_rate = best[0][0]\\nbest_optimizer_choice = int(round(best[0][1]))\\noptimizer_types = [\\'AdamW\\', \\'SGD\\', \\'Adam\\', \\'NAG\\', \\'Nadam\\', \\'RMSprop\\', \\'Adagrad\\']\\nbest_optimizer = optimizer_types[best_optimizer_choice]\\n\\nprint(f\"Best learning rate: {best_learning_rate}, Best optimizer: {best_optimizer}\")'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, DebertaForSequenceClassification\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "from niapy.algorithms.basic import GreyWolfOptimizer,FireflyAlgorithm, HarrisHawksOptimization, MonarchButterflyOptimization, LionOptimizationAlgorithm, BatAlgorithm, BeesAlgorithm, BacterialForagingOptimization, CoralReefsOptimization,  CuckooSearch, CatSwarmOptimization, ForestOptimizationAlgorithm, GlowwormSwarmOptimization, MothFlameOptimizer, ParticleSwarmOptimization\n",
    "\n",
    "# Define GWO problem for hyperparameter optimization\n",
    "class HyperparameterOptimization(Problem):\n",
    "    def __init__(self):\n",
    "        # Dimension 2: learning rate and optimizer type\n",
    "        super().__init__(dimension=2, lower=[1e-6, 0], upper=[1e-3, 6], type='float')\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        learning_rate = x[0]\n",
    "        optimizer_choice = int(round(x[1]))\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_choice == 0:\n",
    "            optimizer_class = torch.optim.AdamW\n",
    "        elif optimizer_choice == 1:\n",
    "            optimizer_class = torch.optim.SGD\n",
    "        elif optimizer_choice == 2:\n",
    "            optimizer_class = torch.optim.Adam\n",
    "        elif optimizer_choice == 3:\n",
    "            optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_choice == 4:\n",
    "            optimizer_class = torch.optim.NAdam\n",
    "        elif optimizer_choice == 5:\n",
    "            optimizer_class = torch.optim.RMSprop\n",
    "        elif optimizer_choice == 6:\n",
    "            optimizer_class = torch.optim.Adagrad\n",
    "\n",
    "\n",
    "\n",
    "        # Load the DeBERTa model\n",
    "        model = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",\n",
    "            num_labels=2,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        model.cuda()\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_dataloader) * 1  # 1 epoch for quick evaluation\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        return avg_train_loss\n",
    "\n",
    "# Run GWO for hyperparameter optimization\n",
    "task = Task(problem=HyperparameterOptimization(), max_evals=10)\n",
    "algo = MonarchButterflyOptimization()\n",
    "best = algo.run(task)\n",
    "\n",
    "best_learning_rate = best[0][0]\n",
    "best_optimizer_choice = int(round(best[0][1]))\n",
    "optimizer_types = ['AdamW', 'SGD', 'Adam', 'NAG', 'Nadam', 'RMSprop', 'Adagrad']\n",
    "best_optimizer = optimizer_types[best_optimizer_choice]\n",
    "\n",
    "print(f\"Best learning rate: {best_learning_rate}, Best optimizer: {best_optimizer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "HKS0MEShz8Y-",
    "outputId": "4daa80b6-5cd7-49dc-c327-60d64bd3d488"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'best_learning_rate = best[0][0]\\nbest_optimizer_choice = int(round(best[0][1]))\\noptimizer_types = [\\'AdamW\\', \\'SGD\\', \\'Adam\\', \\'NAG\\', \\'Nadam\\', \\'RMSprop\\', \\'Adagrad\\']\\nbest_optimizer = optimizer_types[best_optimizer_choice]\\n\\nprint(f\"Best learning rate: {best_learning_rate}, Best optimizer: {best_optimizer}\")'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_learning_rate = best[0][0]\n",
    "best_optimizer_choice = int(round(best[0][1]))\n",
    "optimizer_types = ['AdamW', 'SGD', 'Adam', 'NAG', 'Nadam', 'RMSprop', 'Adagrad']\n",
    "best_optimizer = optimizer_types[best_optimizer_choice]\n",
    "\n",
    "print(f\"Best learning rate: {best_learning_rate}, Best optimizer: {best_optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "-2abq5SE5Fxr",
    "outputId": "15d199b5-7d47-4da2-914a-0fd422e152ab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'import torch\\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\\nfrom transformers import DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\\nfrom sklearn.metrics import classification_report\\nfrom niapy.task import Task\\nfrom niapy.problems import Problem\\nfrom niapy.algorithms.basic import GreyWolfOptimizer,MonarchButterflyOptimization\\n\\n# Assuming you have already defined train_inputs, train_masks, train_labels,\\n# validation_inputs, validation_masks, validation_labels, test_inputs, test_masks, test_labels\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n# Define GWO problem for hyperparameter optimization\\nclass HyperparameterOptimization(Problem):\\n    def __init__(self):\\n        super().__init__(dimension=2, lower=[1e-6, 0], upper=[1e-3, 6], type=\\'float\\')\\n\\n    def _evaluate(self, x):\\n        learning_rate = x[0]\\n        optimizer_choice = int(round(x[1]))\\n\\n        # Choose optimizer\\n        if optimizer_choice == 0:\\n            optimizer_class = torch.optim.AdamW\\n        elif optimizer_choice == 1:\\n            optimizer_class = torch.optim.SGD\\n        elif optimizer_choice == 2:\\n            optimizer_class = torch.optim.Adam\\n        elif optimizer_choice == 3:\\n            optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\\n        elif optimizer_choice == 4:\\n            optimizer_class = torch.optim.Nadam\\n        elif optimizer_choice == 5:\\n            optimizer_class = torch.optim.RMSprop\\n        elif optimizer_choice == 6:\\n            optimizer_class = torch.optim.Adagrad\\n        else:\\n            return float(\\'inf\\')  # Invalid optimizer choice\\n\\n        batch_size = 16  # Fixed batch size for simplicity\\n\\n        # Define DataLoader\\n        train_data = TensorDataset(train_inputs, train_masks, train_labels)\\n        train_sampler = SequentialSampler(train_data)\\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\\n\\n        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\n        validation_sampler = SequentialSampler(validation_data)\\n        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\\n\\n        # Load the DeBERTa model\\n        model = DebertaForSequenceClassification.from_pretrained(\\n            \"microsoft/deberta-base\",\\n            num_labels=2,\\n            output_attentions=False,\\n            output_hidden_states=False,\\n        )\\n        model.to(device)\\n\\n        # Optimizer and scheduler\\n        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\\n        total_steps = len(train_dataloader) * 1  # 1 epoch for quick evaluation\\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\\n\\n        # Training\\n        model.train()\\n        total_loss = 0\\n        for batch in train_dataloader:\\n            b_input_ids = batch[0].to(device)\\n            b_input_mask = batch[1].to(device)\\n            b_labels = batch[2].to(device)\\n            model.zero_grad()\\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\\n            loss = outputs.loss\\n            total_loss += loss.item()\\n            loss.backward()\\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n            optimizer.step()\\n            scheduler.step()\\n\\n        avg_train_loss = total_loss / len(train_dataloader)\\n\\n        return avg_train_loss  # Only return the loss for the optimizer\\n\\n# Define a function for testing with the best hyperparameters\\ndef test_model(model, test_dataloader):\\n    model.eval()\\n    predictions = []\\n    true_labels = []\\n\\n    for batch in test_dataloader:\\n        b_input_ids = batch[0].to(device)\\n        b_input_mask = batch[1].to(device)\\n        b_labels = batch[2].to(device)\\n\\n        with torch.no_grad():\\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\\n\\n        logits = outputs.logits\\n        _, predicted_labels = torch.max(logits, dim=1)\\n\\n        predictions.extend(predicted_labels.cpu().numpy())\\n        true_labels.extend(b_labels.cpu().numpy())\\n\\n    report = classification_report(true_labels, predictions)\\n    return report\\n\\n# Run GWO for hyperparameter optimization\\ntask = Task(problem=HyperparameterOptimization(), max_evals=1)\\nalgo = MonarchButterflyOptimization()\\nbest_result = algo.run(task)\\n\\nprint(f\"Best learning rate: {best_result[0][0]}, Best optimizer choice: {int(round(best_result[0][1]))}\")\\n\\n# Assuming you have test_inputs, test_masks, test_labels prepared similarly to train and validation sets\\nbatch_size = 16  # Ensure consistent batch size\\ntest_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\ntest_sampler = SequentialSampler(test_data)\\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\\n\\n# Test the best model\\n# Recreate the best model and optimizer\\nbest_learning_rate = best_result[0][0]\\nbest_optimizer_choice = int(round(best_result[0][1]))\\nif best_optimizer_choice == 0:\\n    best_optimizer_class = torch.optim.AdamW\\nelif best_optimizer_choice == 1:\\n    best_optimizer_class = torch.optim.SGD\\nelif best_optimizer_choice == 2:\\n    best_optimizer_class = torch.optim.Adam\\nelif best_optimizer_choice == 3:\\n    best_optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\\nelif best_optimizer_choice == 4:\\n    best_optimizer_class = torch.optim.Nadam\\nelif best_optimizer_choice == 5:\\n    best_optimizer_class = torch.optim.RMSprop\\nelif best_optimizer_choice == 6:\\n    best_optimizer_class = torch.optim.Adagrad\\n\\nbest_model = DebertaForSequenceClassification.from_pretrained(\\n    \"microsoft/deberta-base\",\\n    num_labels=2,\\n    output_attentions=False,\\n    output_hidden_states=False,\\n)\\nbest_model.to(device)\\nbest_optimizer = best_optimizer_class(best_model.parameters(), lr=best_learning_rate)\\n\\n# Test the best model\\ntest_report = test_model(best_model, validation_dataloader)\\nprint(\"Test Report:\")\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from transformers import DebertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "from niapy.algorithms.basic import GreyWolfOptimizer,MonarchButterflyOptimization\n",
    "\n",
    "# Assuming you have already defined train_inputs, train_masks, train_labels,\n",
    "# validation_inputs, validation_masks, validation_labels, test_inputs, test_masks, test_labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define GWO problem for hyperparameter optimization\n",
    "class HyperparameterOptimization(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(dimension=2, lower=[1e-6, 0], upper=[1e-3, 6], type='float')\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        learning_rate = x[0]\n",
    "        optimizer_choice = int(round(x[1]))\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_choice == 0:\n",
    "            optimizer_class = torch.optim.AdamW\n",
    "        elif optimizer_choice == 1:\n",
    "            optimizer_class = torch.optim.SGD\n",
    "        elif optimizer_choice == 2:\n",
    "            optimizer_class = torch.optim.Adam\n",
    "        elif optimizer_choice == 3:\n",
    "            optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_choice == 4:\n",
    "            optimizer_class = torch.optim.Nadam\n",
    "        elif optimizer_choice == 5:\n",
    "            optimizer_class = torch.optim.RMSprop\n",
    "        elif optimizer_choice == 6:\n",
    "            optimizer_class = torch.optim.Adagrad\n",
    "        else:\n",
    "            return float('inf')  # Invalid optimizer choice\n",
    "\n",
    "        batch_size = 16  # Fixed batch size for simplicity\n",
    "\n",
    "        # Define DataLoader\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = SequentialSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Load the DeBERTa model\n",
    "        model = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",\n",
    "            num_labels=2,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_dataloader) * 1  # 1 epoch for quick evaluation\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        return avg_train_loss  # Only return the loss for the optimizer\n",
    "\n",
    "# Define a function for testing with the best hyperparameters\n",
    "def test_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "        true_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    return report\n",
    "\n",
    "# Run GWO for hyperparameter optimization\n",
    "task = Task(problem=HyperparameterOptimization(), max_evals=1)\n",
    "algo = MonarchButterflyOptimization()\n",
    "best_result = algo.run(task)\n",
    "\n",
    "print(f\"Best learning rate: {best_result[0][0]}, Best optimizer choice: {int(round(best_result[0][1]))}\")\n",
    "\n",
    "# Assuming you have test_inputs, test_masks, test_labels prepared similarly to train and validation sets\n",
    "batch_size = 16  # Ensure consistent batch size\n",
    "test_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Test the best model\n",
    "# Recreate the best model and optimizer\n",
    "best_learning_rate = best_result[0][0]\n",
    "best_optimizer_choice = int(round(best_result[0][1]))\n",
    "if best_optimizer_choice == 0:\n",
    "    best_optimizer_class = torch.optim.AdamW\n",
    "elif best_optimizer_choice == 1:\n",
    "    best_optimizer_class = torch.optim.SGD\n",
    "elif best_optimizer_choice == 2:\n",
    "    best_optimizer_class = torch.optim.Adam\n",
    "elif best_optimizer_choice == 3:\n",
    "    best_optimizer_class = lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
    "elif best_optimizer_choice == 4:\n",
    "    best_optimizer_class = torch.optim.Nadam\n",
    "elif best_optimizer_choice == 5:\n",
    "    best_optimizer_class = torch.optim.RMSprop\n",
    "elif best_optimizer_choice == 6:\n",
    "    best_optimizer_class = torch.optim.Adagrad\n",
    "\n",
    "best_model = DebertaForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-base\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "best_model.to(device)\n",
    "best_optimizer = best_optimizer_class(best_model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Test the best model\n",
    "test_report = test_model(best_model, validation_dataloader)\n",
    "print(\"Test Report:\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "x5WPtb3yZzfp",
    "outputId": "2919787b-a59b-4f70-e05e-507b240b6666"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'import torch\\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\\nfrom transformers import DebertaForSequenceClassification, DebertaConfig, get_linear_schedule_with_warmup\\nfrom sklearn.metrics import f1_score\\nfrom niapy.task import Task\\nfrom niapy.problems import Problem\\nfrom niapy.algorithms.basic import MonarchButterflyOptimization\\n\\n# Assuming you have already defined train_inputs, train_masks, train_labels,\\n# validation_inputs, validation_masks, validation_labels\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n# Define GWO problem for hyperparameter optimization\\nclass HyperparameterOptimization(Problem):\\n    def __init__(self):\\n        super().__init__(dimension=8, lower=[128, 1, 1, 512, 0.1, 1e-6, 1, 0.9], upper=[1024, 24, 24, 4096, 0.5, 1e-2, 1000, 0.999], type=\\'float\\')\\n\\n    def _evaluate(self, x):\\n        params = {\\n            \"hidden_size\": int(round(x[0])),\\n            \"num_hidden_layers\": int(round(x[1])),\\n            \"num_attention_heads\": int(round(x[2])),\\n            \"intermediate_size\": int(round(x[3])),\\n            \"hidden_dropout_prob\": x[4],\\n            \"learning_rate\": x[5],\\n            \"num_train_epochs\": int(round(x[6])),\\n            \"adam_beta2\": x[7],\\n        }\\n\\n        # Ensure hidden_size is divisible by num_attention_heads\\n        if params[\"hidden_size\"] % params[\"num_attention_heads\"] != 0:\\n            return float(\\'inf\\')  # Return a large value to avoid these invalid configurations\\n\\n        batch_size = 16  # Fixed batch size for simplicity\\n\\n        # Define DataLoader\\n        train_data = TensorDataset(train_inputs, train_masks, train_labels)\\n        train_sampler = SequentialSampler(train_data)\\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\\n\\n        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\\n        validation_sampler = SequentialSampler(validation_data)\\n        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\\n\\n        # Load the DeBERTa model configuration\\n        model_config = DebertaConfig(\\n            hidden_size=params[\"hidden_size\"],\\n            num_hidden_layers=params[\"num_hidden_layers\"],\\n            num_attention_heads=params[\"num_attention_heads\"],\\n            intermediate_size=params[\"intermediate_size\"],\\n            hidden_dropout_prob=params[\"hidden_dropout_prob\"],\\n        )\\n\\n        # Load the DeBERTa model\\n        model = DebertaForSequenceClassification.from_pretrained(\\n            \"microsoft/deberta-base\",\\n            config=model_config,\\n            ignore_mismatched_sizes=True  # Ignore size mismatches\\n        )\\n        model.to(device)\\n\\n        # Optimizer and scheduler\\n        optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"], betas=(0.9, params[\"adam_beta2\"]))\\n        total_steps = len(train_dataloader) * params[\"num_train_epochs\"]\\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\\n\\n        # Training\\n        model.train()\\n        total_loss = 0\\n        for epoch in range(params[\"num_train_epochs\"]):\\n            for batch in train_dataloader:\\n                b_input_ids = batch[0].to(device)\\n                b_input_mask = batch[1].to(device)\\n                b_labels = batch[2].to(device)\\n                model.zero_grad()\\n                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\\n                loss = outputs.loss\\n                total_loss += loss.item()\\n                loss.backward()\\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n                optimizer.step()\\n                scheduler.step()\\n\\n        avg_train_loss = total_loss / (len(train_dataloader) * params[\"num_train_epochs\"])\\n\\n        # Evaluation on validation set\\n        model.eval()\\n        predictions = []\\n        true_labels = []\\n\\n        for batch in validation_dataloader:\\n            b_input_ids = batch[0].to(device)\\n            b_input_mask = batch[1].to(device)\\n            b_labels = batch[2].to(device)\\n\\n            with torch.no_grad():\\n                outputs = model(b_input_ids, attention_mask=b_input_mask)\\n\\n            logits = outputs.logits\\n            _, predicted_labels = torch.max(logits, dim=1)\\n\\n            predictions.extend(predicted_labels.cpu().numpy())\\n            true_labels.extend(b_labels.cpu().numpy())\\n\\n        f1 = f1_score(true_labels, predictions)\\n        return -f1  # Minimize negative F1 score\\n\\n# Run GWO for hyperparameter optimization\\ntask = Task(problem=HyperparameterOptimization(), max_evals=10)\\nalgo = MonarchButterflyOptimization()\\nbest_result = algo.run(task)\\n\\n# Print the best hyperparameters found\\nbest_params_dict = {\\n    \"hidden_size\": int(round(best_result[0][0])),\\n    \"num_hidden_layers\": int(round(best_result[0][1])),\\n    \"num_attention_heads\": int(round(best_result[0][2])),\\n    \"intermediate_size\": int(round(best_result[0][3])),\\n    \"hidden_dropout_prob\": best_result[0][4],\\n    \"learning_rate\": best_result[0][5],\\n    \"num_train_epochs\": int(round(best_result[0][6])),\\n    \"adam_beta2\": best_result[0][7],\\n}\\n\\nprint(f\"Best hyperparameters: {best_params_dict}\")'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from transformers import DebertaForSequenceClassification, DebertaConfig, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from niapy.task import Task\n",
    "from niapy.problems import Problem\n",
    "from niapy.algorithms.basic import MonarchButterflyOptimization\n",
    "\n",
    "# Assuming you have already defined train_inputs, train_masks, train_labels,\n",
    "# validation_inputs, validation_masks, validation_labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define GWO problem for hyperparameter optimization\n",
    "class HyperparameterOptimization(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(dimension=8, lower=[128, 1, 1, 512, 0.1, 1e-6, 1, 0.9], upper=[1024, 24, 24, 4096, 0.5, 1e-2, 1000, 0.999], type='float')\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        params = {\n",
    "            \"hidden_size\": int(round(x[0])),\n",
    "            \"num_hidden_layers\": int(round(x[1])),\n",
    "            \"num_attention_heads\": int(round(x[2])),\n",
    "            \"intermediate_size\": int(round(x[3])),\n",
    "            \"hidden_dropout_prob\": x[4],\n",
    "            \"learning_rate\": x[5],\n",
    "            \"num_train_epochs\": int(round(x[6])),\n",
    "            \"adam_beta2\": x[7],\n",
    "        }\n",
    "\n",
    "        # Ensure hidden_size is divisible by num_attention_heads\n",
    "        if params[\"hidden_size\"] % params[\"num_attention_heads\"] != 0:\n",
    "            return float('inf')  # Return a large value to avoid these invalid configurations\n",
    "\n",
    "        batch_size = 16  # Fixed batch size for simplicity\n",
    "\n",
    "        # Define DataLoader\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = SequentialSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Load the DeBERTa model configuration\n",
    "        model_config = DebertaConfig(\n",
    "            hidden_size=params[\"hidden_size\"],\n",
    "            num_hidden_layers=params[\"num_hidden_layers\"],\n",
    "            num_attention_heads=params[\"num_attention_heads\"],\n",
    "            intermediate_size=params[\"intermediate_size\"],\n",
    "            hidden_dropout_prob=params[\"hidden_dropout_prob\"],\n",
    "        )\n",
    "\n",
    "        # Load the DeBERTa model\n",
    "        model = DebertaForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-base\",\n",
    "            config=model_config,\n",
    "            ignore_mismatched_sizes=True  # Ignore size mismatches\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"], betas=(0.9, params[\"adam_beta2\"]))\n",
    "        total_steps = len(train_dataloader) * params[\"num_train_epochs\"]\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for epoch in range(params[\"num_train_epochs\"]):\n",
    "            for batch in train_dataloader:\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                model.zero_grad()\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / (len(train_dataloader) * params[\"num_train_epochs\"])\n",
    "\n",
    "        # Evaluation on validation set\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        return -f1  # Minimize negative F1 score\n",
    "\n",
    "# Run GWO for hyperparameter optimization\n",
    "task = Task(problem=HyperparameterOptimization(), max_evals=10)\n",
    "algo = MonarchButterflyOptimization()\n",
    "best_result = algo.run(task)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "best_params_dict = {\n",
    "    \"hidden_size\": int(round(best_result[0][0])),\n",
    "    \"num_hidden_layers\": int(round(best_result[0][1])),\n",
    "    \"num_attention_heads\": int(round(best_result[0][2])),\n",
    "    \"intermediate_size\": int(round(best_result[0][3])),\n",
    "    \"hidden_dropout_prob\": best_result[0][4],\n",
    "    \"learning_rate\": best_result[0][5],\n",
    "    \"num_train_epochs\": int(round(best_result[0][6])),\n",
    "    \"adam_beta2\": best_result[0][7],\n",
    "}\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params_dict}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEtiO1-WDnpd",
    "outputId": "68730566-4141-48aa-f47b-ac90dbe626ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaForSequenceClassification(\n",
       "  (deberta): DebertaModel(\n",
       "    (embeddings): DebertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
       "      (LayerNorm): DebertaLayerNorm()\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaLayer(\n",
       "          (attention): DebertaAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DebertaLayerNorm()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(1024, 768)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import DebertaForSequenceClassification, DebertaTokenizer\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
    "# linear classification layer on top.\n",
    "model = DebertaForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-base\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yIjq6uTDnsc",
    "outputId": "fa113ff5-ea1b-4de0-e074-f18f51f40936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 30 14:22:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   63C    P0              30W /  70W |    693MiB / 15360MiB |     17%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_phaNWwREB5b",
    "outputId": "43d7fe0b-af4d-4f2a-ed1b-7380dfb923be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 200 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "deberta.embeddings.word_embeddings.weight               (50265, 768)\n",
      "deberta.embeddings.LayerNorm.weight                           (768,)\n",
      "deberta.embeddings.LayerNorm.bias                             (768,)\n",
      "deberta.encoder.layer.0.attention.self.q_bias                 (768,)\n",
      "deberta.encoder.layer.0.attention.self.v_bias                 (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "deberta.encoder.layer.0.attention.self.in_proj.weight    (2304, 768)\n",
      "deberta.encoder.layer.0.attention.self.pos_proj.weight    (768, 768)\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.weight   (768, 768)\n",
      "deberta.encoder.layer.0.attention.self.pos_q_proj.bias        (768,)\n",
      "deberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "deberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "deberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "deberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "deberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "deberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "deberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "deberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "deberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "deberta.encoder.layer.1.attention.self.q_bias                 (768,)\n",
      "deberta.encoder.layer.1.attention.self.v_bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6XvGOlNEB6n"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=8.284448858002006e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM4Tju6TEB9B",
    "outputId": "459c2117-7d6c-437c-b326-c74782f6ee47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x7b1aff6baa70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gTAwpRHECAi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfT1CHuBECB7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9FS41a_ECFK",
    "outputId": "d8d539b3-adbf-4c53-9294-a368211d65c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.5339\n",
      "  Average training accuracy: 0.7215\n",
      "  Training epoch took: 0:01:26\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.8625\n",
      "  Validation Loss: 0.3274\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.2811\n",
      "  Average training accuracy: 0.8979\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9062\n",
      "  Validation Loss: 0.3046\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.1257\n",
      "  Average training accuracy: 0.9604\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.8812\n",
      "  Validation Loss: 0.3480\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0259\n",
      "  Average training accuracy: 0.9910\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 5 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0053\n",
      "  Average training accuracy: 0.9979\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 6 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0041\n",
      "  Average training accuracy: 0.9986\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 7 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0052\n",
      "  Average training accuracy: 0.9979\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 8 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0036\n",
      "  Average training accuracy: 0.9993\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 9 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0056\n",
      "  Average training accuracy: 0.9979\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 10 / 4 ========\n",
      "Training...\n",
      "  Average training loss: 0.0040\n",
      "  Average training accuracy: 0.9993\n",
      "  Training epoch took: 0:01:25\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.9000\n",
      "  Validation Loss: 0.4691\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss and accuracy after each epoch for plotting\n",
    "train_loss_values = []\n",
    "train_accuracy_values = []\n",
    "validation_loss_values = []\n",
    "validation_accuracy_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, 10):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Clear gradients before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_accuracy += accuracy_score(np.argmax(preds, axis=1), label_ids)\n",
    "\n",
    "    # Calculate the average loss and accuracy over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = total_accuracy / len(train_dataloader)\n",
    "\n",
    "    # Store the loss and accuracy values for plotting\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "    train_accuracy_values.append(avg_train_accuracy)\n",
    "\n",
    "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    print(\"  Average training accuracy: {0:.4f}\".format(avg_train_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "      # Telling the model not to compute or store gradients, saving memory and\n",
    "      # speeding up validation\n",
    "      with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which\n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here:\n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        # Convert logits to PyTorch tensor if it's a NumPy array\n",
    "        logits = torch.from_numpy(logits)\n",
    "\n",
    "        # Ensure label type compatibility\n",
    "        b_labels = b_labels.long()\n",
    "        logits = logits.to(b_labels.device)\n",
    "\n",
    "        # Calculate the evaluation loss for this batch and accumulate it\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, 2), b_labels.view(-1))\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    # Store the validation loss and accuracy values\n",
    "    validation_loss_values.append(eval_loss / len(validation_dataloader))\n",
    "    validation_accuracy_values.append(eval_accuracy / len(validation_dataloader))\n",
    "\n",
    "\n",
    "    # Report the final accuracy, loss, and time taken for this validation run.\n",
    "    print(\"  Validation Accuracy: {0:.4f}\".format(eval_accuracy/len(validation_dataloader)))\n",
    "    print(\"  Validation Loss: {0:.4f}\".format(eval_loss/len(validation_dataloader)))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
